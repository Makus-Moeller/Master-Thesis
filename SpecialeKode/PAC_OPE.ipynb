{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDR\n",
    "# Example data (replace with your actual data)\n",
    "trajectories = [\n",
    "    {\"states\": [s1, s2, s3], \"actions\": [a1, a2, a3], \"rewards\": [r1, r2, r3], \"next_states\": [s2, s3, s4]},\n",
    "    # ... more trajectories ...\n",
    "]\n",
    "\n",
    "# Define target and behavior policies (replace with your actual policies)\n",
    "def target_policy(state):\n",
    "    # Your target policy logic\n",
    "    return action\n",
    "\n",
    "def behavior_policy(state):\n",
    "    # Your behavior policy logic\n",
    "    return action\n",
    "\n",
    "# Doubly Robust Estimator (MDR)\n",
    "def doubly_robust_estimator(trajectories):\n",
    "    total_q_values = 0\n",
    "    for traj in trajectories:\n",
    "        importance_weight = 1.0\n",
    "        q_value = 0\n",
    "        for t in range(len(traj[\"states\"])):\n",
    "            importance_weight *= target_policy(traj[\"states\"][t]) / behavior_policy(traj[\"states\"][t])\n",
    "            q_value += importance_weight * traj[\"rewards\"][t]\n",
    "        total_q_values += q_value\n",
    "    mdr_estimate = total_q_values / len(trajectories)\n",
    "    return mdr_estimate\n",
    "\n",
    "# Example usage\n",
    "mdr_result = doubly_robust_estimator(trajectories)\n",
    "print(f\"MDR estimate: {mdr_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MR\n",
    "# Example data (same as above)\n",
    "trajectories = [\n",
    "    {\"states\": [s1, s2, s3], \"actions\": [a1, a2, a3], \"rewards\": [r1, r2, r3], \"next_states\": [s2, s3, s4]},\n",
    "    # ... more trajectories ...\n",
    "]\n",
    "\n",
    "# Marginal Ratio (MR) Estimator\n",
    "def marginal_ratio_estimator(trajectories):\n",
    "    total_reweighted_rewards = 0\n",
    "    for traj in trajectories:\n",
    "        importance_weight = 1.0\n",
    "        for t in range(len(traj[\"states\"])):\n",
    "            importance_weight *= target_policy(traj[\"states\"][t]) / behavior_policy(traj[\"states\"][t])\n",
    "            reweighted_reward = importance_weight * traj[\"rewards\"][t]\n",
    "            total_reweighted_rewards += reweighted_reward\n",
    "    mr_estimate = total_reweighted_rewards / len(trajectories)\n",
    "    return mr_estimate\n",
    "\n",
    "# Example usage\n",
    "mr_result = marginal_ratio_estimator(trajectories)\n",
    "print(f\"MR estimate: {mr_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
